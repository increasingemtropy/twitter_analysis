{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKcajepNTISU"
   },
   "source": [
    "# Demo of 'live' twitter sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fHz98-Ybx-K"
   },
   "source": [
    "This is a work in progress project using tweepy and textblob to perform 'live' twitter sentiment analysis.\n",
    "\n",
    "The word 'live' is in quotes because, while we are catching live tweets, analysing the sentiment and writing it into a csv file in real time, the plotting and visualisation are all done post-hoc from the datafile, so by the time you get to see the data, it's a little out of date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiw84mpORYLt"
   },
   "source": [
    "## Import dependencies\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYdcQHjCbnCt"
   },
   "source": [
    "*   tweepy - Interact with twitter API\n",
    "*   textblob - Sentiment analysis\n",
    "*   pandas - Create dataframe\n",
    "*   matplotlib - Plotting\n",
    "*   re - Regex for text cleaning\n",
    "*   csv - read/write CSVs\n",
    "*   nltk - Natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMfxHmPI4QYl"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "from  textblob import TextBlob\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JCfiK0bTSWk"
   },
   "source": [
    "Might need to install 'punkt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBOyu9sURWTo",
    "outputId": "66cab776-a40f-4fe7-8ebd-fc5429d7a81d"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_-ew8srTpt7"
   },
   "source": [
    "## Set up Twitter API access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St2fqKN7Taey"
   },
   "source": [
    "Insert your own Twitter API keys and tokens here ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tKsXU_BvqwB"
   },
   "outputs": [],
   "source": [
    "api_key = \"\"\n",
    "api_key_secret = \"\"\n",
    "api_token = \"\"\n",
    "api_token_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_tIPia1wzWZ"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(api_token, api_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGQWsuJbT1tN"
   },
   "source": [
    "Run this just to check everything is working. If so, you can read your twitter feed here now! How (in)convenient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8rnsWv1w-5R"
   },
   "outputs": [],
   "source": [
    "tweets = api.home_timeline()\n",
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4tKMFfjUGoj"
   },
   "source": [
    "## Function and class definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq0bPXw7bdlm"
   },
   "source": [
    "This part is still very messy and many functions should be extracted and modularised outside of the stream listener. It should be considered a proof of concept / work in progress for now.\n",
    "\n",
    "Currently, we create a data file, create a stream listener and then inside that stream listener, we do some sentiment analysis using textblob and then add that sentiment (+1 is good, -1 is bad, 0 is neutral) to the running counts for each of the keywords we identified. and then write it into a file. \n",
    "\n",
    "- Note 1: we only count here if the tweet mentions only one of the agents, since then we don't get confused with things like \"I hate coke but I love pepsi\" accidentally counting negative for pepsi and positive for coke etc.\n",
    "- Note 2: we can see that a LOT of data is actually available to us from the stream listener, here we are only using the text, but in general it might be useful to look at the user description, location etc. Even the background colour of their user profile! There are more things available as well, although I am not yet sure what the complete set of methods and variables is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_S20VjfW8xY"
   },
   "source": [
    "### Create stream listener and write into data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZYEFcIgLrnS"
   },
   "outputs": [],
   "source": [
    "trump = 0\n",
    "biden = 0\n",
    "\n",
    "header_name = ['Trump', 'Biden']\n",
    "with open('sentiment.csv', 'w') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "    writer.writeheader()\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        description = status.user.description\n",
    "        loc = status.user.location\n",
    "        text = status.text\n",
    "        coords = status.coordinates\n",
    "        name = status.user.screen_name\n",
    "        user_created = status.user.created_at\n",
    "        followers = status.user.followers_count\n",
    "        id_str = status.id_str\n",
    "        created = status.created_at\n",
    "        retweets = status.retweet_count\n",
    "        bg_color = status.user.profile_background_color\n",
    "      \n",
    "        blob = TextBlob(text)\n",
    "        sent = blob.sentiment\n",
    "        polarity = sent.polarity\n",
    "        subjectivity = sent.subjectivity\n",
    "\n",
    "      \n",
    "        text = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', r' ', text)\n",
    "        text = re.sub(r'RT',' ', text)\n",
    "        blob = TextBlob(text)\n",
    "      \n",
    "        global trump\n",
    "        global biden\n",
    "            \n",
    "        trump_sentiment = 0\n",
    "        biden_sentiment = 0\n",
    "        \n",
    "        for sent in blob.sentences:\n",
    "            if \"Trump\" in sent and \"Biden\" not in sent:\n",
    "                trump_sentiment = trump_sentiment + sent.sentiment.polarity # could also use sentiment.subjectivity\n",
    "            if \"Biden\" in sent and \"Trump\" not in sent:\n",
    "                biden_sentiment = biden_sentiment + sent.sentiment.polarity # could also use sentiment.subjectivity\n",
    "        \n",
    "        trump = trump + trump_sentiment\n",
    "        biden = biden + biden_sentiment\n",
    "        \n",
    "        with open('sentiment.csv', 'a') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=header_name)\n",
    "            info = {\n",
    "                'Trump': trump,\n",
    "                'Biden': biden\n",
    "            }\n",
    "            writer.writerow(info)\n",
    "\n",
    "        # uncomment these lines if you want to flood the console with garbage/tweets.      \n",
    "        #print(text)\n",
    "        #print()\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RaWd0KsXbYCV"
   },
   "source": [
    "### Run stream listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXTvRnywYL9v"
   },
   "source": [
    "Run the stream listener for as long as you like and gather data!\n",
    "\n",
    "N.B. you will need to manually interrupt this cell when you are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_RC943wLyyq"
   },
   "outputs": [],
   "source": [
    "stream_listener = StreamListener()\n",
    "stream = tweepy.Stream(auth=auth, listener=stream_listener)\n",
    "stream.filter(track=[\"trump\", \"biden\", \"joe biden\", \"donald trump\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUBzFbseZeLW"
   },
   "source": [
    "## Plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm520imbZh7B"
   },
   "source": [
    "Read the data in from a CSV file we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tJgtcqx-QlA"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/sentiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0m2iXl8Ztfw"
   },
   "source": [
    "Plot a graph.\n",
    "\n",
    "Many of these plotting options are quite idiosyncratic to my own personal tastes. Feel free to modify/tinker with it and remove the watermark :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JZcUxEc-x6l"
   },
   "outputs": [],
   "source": [
    "MARKERS = 'ov^.psDPx*<>+pDov^.ps'\n",
    "\n",
    "#choose if you want a smoothed plot or not\n",
    "#recommend to only use this if you are gathering 1000+ tweets\n",
    "smoothed = True\n",
    "\n",
    "fig = plt.figure(figsize = (14,7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "if smoothed:\n",
    "    ax.plot(df.index.to_numpy(),((df.fillna(value=0)).rolling(50).sum()/2).to_numpy(),markevery=20,linewidth=3,markersize=0)\n",
    "    ax.set_title('Sentiment of tweets (+ve/-ve)',fontsize=20)\n",
    "else:\n",
    "    ax.plot(df.index.to_numpy(),(df.fillna(value=0)).to_numpy(),markevery=20,linewidth=2,markersize=10)\n",
    "    ax.set_title('Sentiment of tweets (+ve/-ve)',fontsize=20)\n",
    "ax.grid(axis='y', which='major', linewidth=1)\n",
    "ax.grid(axis='y', which='minor', linewidth=0.5)\n",
    "ax.grid(axis='x', which='major', linewidth=1)\n",
    "ax.grid(axis='x', which='minor', linewidth=0.5)\n",
    "for i, line in enumerate(ax.get_lines()):\n",
    "    line.set_marker(MARKERS[i])\n",
    "ax.set_xlabel('Tweets',fontsize=20)\n",
    "ax.set_ylabel('Sentiment',fontsize=20)\n",
    "ax.legend(ax.get_lines(), df.columns,fontsize=15, loc='upper left')\n",
    "ax.yaxis.tick_right()\n",
    "for item in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "if logscale:\n",
    "    ax.set_yscale('log')\n",
    "    locmin = plticker.LogLocator(base=10.0, subs=(1,10 ))\n",
    "    ax.yaxis.set_major_formatter(plticker.FormatStrFormatter(\"%.0f\"))\n",
    "    ax.yaxis.set_major_locator(locmin)\n",
    "#ax.xaxis.set_major_locator(plticker.AutoLocator())\n",
    "ax.xaxis.set_minor_locator(plticker.AutoMinorLocator())\n",
    "ax.text(0.99, 0.025, \"(c) E. Maitland 2020\", transform=ax.transAxes, fontsize=8.5, va='top', ha='right')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tweepy_twitter_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
